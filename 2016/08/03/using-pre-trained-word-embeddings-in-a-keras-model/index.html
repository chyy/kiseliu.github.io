<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Helvetica Neue:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Deep Learning,Keras," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="在本教程中，我们将带你体验使用预先训练好的词嵌入和卷积神经网络解决文本分类问题的过程。
What are word embeddings?“Word embeddings”是一种旨在将语义映射到几何空间的自然语言处理技术，它将字典中的每个词和一个数值向量进行关联，比如任意两个向量间的距离(L2距离或者更常见的余弦距离)表示了两个相关联的词汇间的部分语义关系。由这些向量构成的几何空间叫做embedd">
<meta property="og:type" content="article">
<meta property="og:title" content="Using Pre-Trained Word Embeddings in a Keras Model">
<meta property="og:url" content="http://github.com/kiseliu/2016/08/03/using-pre-trained-word-embeddings-in-a-keras-model/index.html">
<meta property="og:site_name" content="Welcome to kiseliu's homepage">
<meta property="og:description" content="在本教程中，我们将带你体验使用预先训练好的词嵌入和卷积神经网络解决文本分类问题的过程。
What are word embeddings?“Word embeddings”是一种旨在将语义映射到几何空间的自然语言处理技术，它将字典中的每个词和一个数值向量进行关联，比如任意两个向量间的距离(L2距离或者更常见的余弦距离)表示了两个相关联的词汇间的部分语义关系。由这些向量构成的几何空间叫做embedd">
<meta property="og:updated_time" content="2016-09-20T06:17:06.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Using Pre-Trained Word Embeddings in a Keras Model">
<meta name="twitter:description" content="在本教程中，我们将带你体验使用预先训练好的词嵌入和卷积神经网络解决文本分类问题的过程。
What are word embeddings?“Word embeddings”是一种旨在将语义映射到几何空间的自然语言处理技术，它将字典中的每个词和一个数值向量进行关联，比如任意两个向量间的距离(L2距离或者更常见的余弦距离)表示了两个相关联的词汇间的部分语义关系。由这些向量构成的几何空间叫做embedd">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>




  <link rel="canonical" href="http://github.com/kiseliu/2016/08/03/using-pre-trained-word-embeddings-in-a-keras-model/"/>

  <title> Using Pre-Trained Word Embeddings in a Keras Model | Welcome to kiseliu's homepage </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
<!-- hexo-inject:begin --><!-- hexo-inject:end --><script>
  window.fbAsyncInit = function() {
    FB.init({
      appId      : '',
      xfbml      : true,
      version    : 'v2.6'
    });
  };

  (function(d, s, id){
     var js, fjs = d.getElementsByTagName(s)[0];
     if (d.getElementById(id)) {return;}
     js = d.createElement(s); js.id = id;
     js.src = "//connect.facebook.net/en/sdk.js";
     fjs.parentNode.insertBefore(js, fjs);
   }(document, 'script', 'facebook-jssdk'));
</script>











  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Welcome to kiseliu's homepage</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">Here's a dream chaser.</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-search">
          <a href="/search" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-search"></i> <br />
            
            Search
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Using Pre-Trained Word Embeddings in a Keras Model
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-08-03T16:16:24+08:00" content="2016-08-03">
              2016-08-03
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/08/03/using-pre-trained-word-embeddings-in-a-keras-model/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/08/03/using-pre-trained-word-embeddings-in-a-keras-model/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>在本教程中，我们将带你体验使用预先训练好的词嵌入和卷积神经网络解决文本分类问题的过程。</p>
<h4 id="What-are-word-embeddings"><a href="#What-are-word-embeddings" class="headerlink" title="What are word embeddings?"></a>What are word embeddings?</h4><p>“<a href="https://en.wikipedia.org/wiki/Word_embedding" target="_blank" rel="external">Word embeddings</a>”是一种旨在将语义映射到几何空间的自然语言处理技术，它将字典中的每个词和一个数值向量进行关联，比如任意两个向量间的距离(L2距离或者更常见的余弦距离)表示了两个相关联的词汇间的部分语义关系。由这些向量构成的几何空间叫做embedding space。</p>
<p>比如说，”coconut” 和 “polar bear”是语义上很不同的词语，那么一个合理的embedding space会把它们表示成隔得很远的向量。但是”kitchen” 和 “dinner” 是有关系的词语，所以它们应该互相隔的很近。</p>
<p>理想情况下，在一个好的嵌入空间里，从”kitchen” 和 “dinner” 出发的”path” (a vector) 会准确地抓住这两个概念的语义关系。在这种情况下，关系是”where x occurs” , 所以你会期望向量 kitchen - dinner(二个嵌入向量的差，也就是从 kitchen 到 dinner 的路径 )能够表示这种”where x occurs”关系。最基本地，我们应该有向量的相等： dinner + (where x occurs) = kitchen(至少大约相等)。<br>如果这确实成立，我们可以用这种关系向量来回答问题。比如，从一个新向量开始，比如”work”, 然后应用这种关系向量，我们有时会得到对问题”where does work occur?”有意义的回答work + (where x occurs) = office。</p>
<p>Word embeddings 通过将降维技术应用到语料库文本中的词汇间的共现统计的数据集上进行计算。这可以通过神经网络(“word2vec”技术)，或者因子分解做到。</p>
<h4 id="GloVe-word-embeddings"><a href="#GloVe-word-embeddings" class="headerlink" title="GloVe word embeddings"></a>GloVe word embeddings</h4><p>我们将使用GloVe embeddings，你可以在这里了解<a href="http://nlp.stanford.edu/projects/glove/" target="_blank" rel="external">它</a>。GloVe表示词的全局向量表示。它是基于词的共现统计矩阵分解的一种流行的embedding技术。</p>
<p>特别地，我们将使用由2014 dump of English Wikipedia 计算出来的400k个词语的100维的GloVe embeddings。</p>
<h4 id="20-Newsgroup-dataset"><a href="#20-Newsgroup-dataset" class="headerlink" title="20 Newsgroup dataset"></a>20 Newsgroup dataset</h4><p>The task we will try to solve will be to classify posts coming from 20 different newsgroup, into their original 20 categories –the infamous “20 Newsgroup dataset”. You can read about the dataset and download the raw text data <a href="http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html" target="_blank" rel="external">here</a>.</p>
<p>Categories are fairly semantically distinct and thus will have quite different words associated with them. Here are a few sample categories:</p>
<pre><code>comp.sys.ibm.pc.hardware
comp.graphics
comp.os.ms-windows.misc
comp.sys.mac.hardware
comp.windows.x

rec.autos
rec.motorcycles
rec.sport.baseball
rec.sport.hockey
</code></pre><h4 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h4><p>以下是我们将如何解决分类问题：</p>
<ul>
<li>将数据集中所有的文本样本转化成带索引的词语的序列。词语的索引是整数ID。我们只考虑数据中最常出现的前20000个词语，并且我们将把序列截断成最大长度为1000个词语。</li>
<li>prepare an “embedding matrix” which will contain at index i the embedding vector for the word of index i in our word index.</li>
<li>加载这个 embedding matrix 到一个Keras Embedding layer,然后设置为 frozen (它的权重，embedding vectors 在训练阶段都不会被更新)。</li>
<li>构建于1维的卷积神经网络，结束于20个类的softmax输出。</li>
</ul>
<h4 id="Preparing-the-text-data"><a href="#Preparing-the-text-data" class="headerlink" title="Preparing the text data"></a>Preparing the text data</h4><p>首先，我们简单地对文本样本存储的文件夹进行迭代，然后把它们格式化为样本的列表。我们同时也准备样本对应的类别索引列表。</p>
<pre><code>#coding:utf-8
import os

texts = []
labels_index = {}
labels = []
text_data_dir = &quot;/path/20news-bydate/20news-bydate-train&quot;
for name in sorted(os.listdir(text_data_dir)):
    path = os.path.join(text_data_dir, name)
    if os.path.isdir(path):
        label_id = len(labels_index)
        labels_index[name] = label_id
        for fname in sorted(os.listdir(path)):
            if fname.isdigit():
                fpath = os.path.join(path, fname)
                f = open(fpath)
                texts.append(f.read())
                f.close()
                labels.append(label_id)

print &quot;Found %s texts.&quot; % len(texts)
</code></pre><p>然后我们格式化文本样本和标签为tensors，这些tensors可以被喂给神经网络。为了达到此目的，我们需要依赖Keras的工具keras.preprocessing.text.Tokenizer 和keras.preprocessing.sequence.pad_sequences.</p>
<pre><code>from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
MAX_NB_WORDS = 20000
tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(texts)
word_index = tokenizer.word_index
print &quot;Found %s unique tokens.&quot; % len(word_index)
# print word_index

sequences = tokenizer.texts_to_sequences(texts)

MAX_SEQUENCE_LENGTH = 20000
data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
labels = to_categorical(np.asarray(labels))
print(&apos;Shape of data tensor:&apos;, data.shape)
print(&apos;Shape of label tensor:&apos;, labels.shape)


# shuffle数据
indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]

# 然后把数据分成训练集和测试集
VALIDATION_SPLIT = 0.2
nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])
x_train = data[:-nb_validation_samples]
y_train = labels[:-nb_validation_samples]
x_val = data[-nb_validation_samples:]
y_val = labels[-nb_validation_samples:]
</code></pre><h4 id="Preparing-the-Embedding-layer"><a href="#Preparing-the-Embedding-layer" class="headerlink" title="Preparing the Embedding layer"></a>Preparing the Embedding layer</h4><p>Next, we compute an index mapping words to known embeddings, by parsing the data dump of pre-trained embeddings: </p>
<pre><code>#从预训练好的GloVe文件中解析出每个词和它所对应的词向量，并用字典的方式存储
embeddings_index = {}
glove_text = &apos;&apos;
f = open(glove_text)
for line in f:
    values = line.split()
    word = values[0]  #取词
    coefs = np.asarray(values[1:], dtype=&apos;float32&apos;)  #取向量
    embeddings_index[word] = coefs  #将词和对应的向量存到字典里
f.close()
print(&apos;Found %s word vectors.&apos; % len(embeddings_index))
</code></pre><p>此时，我们可以加载 embedding_index字典 和 word_index 来计算 embedding 矩阵：</p>
<pre><code>EMBEDDING_DIM = 100
embeddings_matrix = np.zeros((len(word_index)+1, EMBEDDING_DIM))
for word, i in word_index.items():  #借助字典embeddings_index[词,向量]将&quot;词和索引&quot;转化为&quot;索引和向量&quot;
    embeddings_vector = embeddings_index.get(word)   # 在embedding index 中没有找到的词,其对应的向量会是全0
    if embeddings_vector is not None:
        embeddings_matrix[i] = embeddings_vector
</code></pre><p>我们加载 embedding矩阵 到 Embedding层，注意我们设置trainable=False来防止权重在训练的过程中被更新。</p>
<pre><code>from keras.layers import Embedding
embeddings_layer = Embedding(len(word_index)+1, EMBEDDING_DIM, weights=[embedding_matrix],
                             input_length=MAX_SEQUENCE_LENGTH, trainable=False)
</code></pre><p>Embedding层接收的应该是整数序列，比如一个2D的输入，它的shape值为(samples, indices)，也就是一个samples行，indeces列的矩阵。这些输入序列应该被填充使得每一批的输入数据都有相同的长度(尽管Embedding层可以处理不同长度的序列，如果你不传递一个清晰的input_length参数给该层)。</p>
<p>Embedding层做的事情是将整数输入映射到embedding矩阵中对应的索引的向量。比如序列[1, 2]将被转换为[embeddings<a href="https://en.wikipedia.org/wiki/Word_embedding" target="_blank" rel="external">1</a>, embeddings<a href="http://nlp.stanford.edu/projects/glove/" target="_blank" rel="external">2</a>]，这样对于二维的输入，Embedding层的输出将是一个大小为(samples, sequence_length, embedding_dim)的三维张量积。</p>
<h4 id="Training-a-1D-convnet"><a href="#Training-a-1D-convnet" class="headerlink" title="Training a 1D convnet"></a>Training a 1D convnet</h4><p>最后，我们可以构建一个一层的卷积神经网络来解决我们的分类问题：</p>
<pre><code>from keras.layers import Dense, Input, Flatten
from keras.layers import Conv1D, MaxPooling1D, Embedding
from keras.models import Model

sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=&apos;int32&apos;)
embedded_sequences = embeddings_layer(sequence_input)
x = Conv1D(128, 5, activation=&apos;relu&apos;)(embedded_sequences)
x = MaxPooling1D(5)(x)
x = Conv1D(128, 5, activation=&apos;relu&apos;)(x)
x = MaxPooling1D(5)(x)
x = Conv1D(128, 5, activation=&apos;relu&apos;)(x)
x = MaxPooling1D(35)(x)
x = Flatten()(x)
x = Dense(128, activation=&apos;relu&apos;)(x)
preds = Dense(len(labels_index), activation=&apos;softmax&apos;)(x)

model = Model(sequence_input, preds)
model.compile(loss=&apos;categorical_crossentropy&apos;, optimizer=&apos;rmsprop&apos;, metrics=[&apos;acc&apos;])

model.fit(x_train, y_train, validation_data=(x_val, y_val), nb_epoch=2, batch_size=128)
</code></pre><p>只要经过2次迭代，该模型即可在交叉集上达到95%的分类准确率。通过使用一些正则化机制(比如dropout)训练得更久，或者对Embedding调优，你可能会得到更高的准确率。</p>
<p>我们也可以测试下如果不使用预先训练好的词向量，而是从头开始初始化Embedding层，在训练的过程中学习它的值，准确率会如何？我们只需要用下列的代码替换Embedding层：</p>
<pre><code>embedding_layer = Embedding(len(word_index) + 1,
                            EMBEDDING_DIM,
                            input_length=MAX_SEQUENCE_LENGTH)
</code></pre><p>在2次迭代之后，该方法只能得到90%的交叉验证准确率，比我们前面的模型只迭代一次的准确率低。我们预训练的向量肯定带来了些什么。In general, using pre-trained embeddings is relevant for natural processing tasks were little training data is available (functionally the embeddings act as an injection of outside information which might prove useful for your model).</p>

      
    </div>

    <div>
      
        
      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/deep-learning/" rel="tag">#Deep Learning</a>
          
            <a href="/tags/keras/" rel="tag">#Keras</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/08/02/机器学习算法总结-聚类/" rel="next" title="机器学习算法总结——聚类篇">
                <i class="fa fa-chevron-left"></i> 机器学习算法总结——聚类篇
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/08/05/pyspark-ml库中的classification/" rel="prev" title="PySpark.ml库中的Classification">
                PySpark.ml库中的Classification <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2016/08/03/using-pre-trained-word-embeddings-in-a-keras-model/"
           data-title="Using Pre-Trained Word Embeddings in a Keras Model" data-url="http://github.com/kiseliu/2016/08/03/using-pre-trained-word-embeddings-in-a-keras-model/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/myname.png"
               alt="kiseliu" />
          <p class="site-author-name" itemprop="name">kiseliu</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/">
              <span class="site-state-item-count">29</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/">
                
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">categories</span>
                
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/">
              
                <span class="site-state-item-count">33</span>
                <span class="site-state-item-name">tags</span>
              
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/kiseliu" target="_blank" title="Github">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  Github
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/kiseliu" target="_blank" title="LinkedIn">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  LinkedIn
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/kiseliu" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                  Twitter
                </a>
              </span>
            
          
        </div>

        
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#What-are-word-embeddings"><span class="nav-number">1.</span> <span class="nav-text">What are word embeddings?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GloVe-word-embeddings"><span class="nav-number">2.</span> <span class="nav-text">GloVe word embeddings</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#20-Newsgroup-dataset"><span class="nav-number">3.</span> <span class="nav-text">20 Newsgroup dataset</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Approach"><span class="nav-number">4.</span> <span class="nav-text">Approach</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Preparing-the-text-data"><span class="nav-number">5.</span> <span class="nav-text">Preparing the text data</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Preparing-the-Embedding-layer"><span class="nav-number">6.</span> <span class="nav-text">Preparing the Embedding layer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Training-a-1D-convnet"><span class="nav-number">7.</span> <span class="nav-text">Training a 1D convnet</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">kiseliu</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"kiseliu"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
  






  
  
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  
<script type="text/javascript" async src="//push.zhanzhang.baidu.com/push.js">
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


</body>
</html>
